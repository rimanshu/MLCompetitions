{"cells":[{"metadata":{"_uuid":"7cb79609321312c9f080227e2987cb91b519d831"},"cell_type":"markdown","source":"# Model Building"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc # garbage collector\nfrom scipy.stats import norm\n\n# Visualization\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set(style=\"darkgrid\")\n\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 18\nplt.rcParams['patch.edgecolor'] = 'k'\n\n%matplotlib inline\n\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\npy.init_notebook_mode(connected=True)\n\nimport warnings\nwarnings.filterwarnings('ignore', category = RuntimeWarning)\n\n# Always good to set a seed for reproducibility\nSEED = 7\nnp.random.seed(SEED)\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 50","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# check files\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input/input/\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d75d9d196b03ecce76dc71e5c07225347383441c"},"cell_type":"code","source":"# Load Data\nprint(\"Loading data...\")\ntrain = pd.read_csv('../input/input/land_train.csv')\nprint(\"Train rows and columns\", train.shape)\ntest = pd.read_csv('../input/input/land_test.csv')\nprint(\"Train rows and columns\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6261acdaff117b42bbc5c08cd9f7def239c3831"},"cell_type":"markdown","source":"## 1.  Remove Outliers "},{"metadata":{"trusted":true,"_uuid":"cdc8ca8fe2029f3776a3448179a0639d70114a6b"},"cell_type":"code","source":"def TurkyOutliers(df_out,nameOfFeature,drop=False):\n\n    valueOfFeature = df_out[nameOfFeature]\n    # Calculate Q1 (25th percentile of the data) for the given feature\n    Q1 = np.percentile(valueOfFeature, 25.)\n\n    # Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = np.percentile(valueOfFeature, 75.)\n\n    # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = (Q3-Q1)*1.5\n    # print \"Outlier step:\", step\n    outliers = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].index.tolist()\n    feature_outliers = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].values\n    # df[~((df[nameOfFeature] >= Q1 - step) & (df[nameOfFeature] <= Q3 + step))]\n\n\n    # Remove the outliers, if any were specified\n    print (\"Number of outliers (inc duplicates): {} and outliers: {}\".format(len(outliers), feature_outliers))\n    if drop:\n        good_data = df_out.drop(df_out.index[outliers]).reset_index(drop = True)\n        print (\"New dataset with removed outliers has {} samples with {} features each.\".format(*good_data.shape))\n        return good_data\n    else: \n        print (\"Nothing happens, df.shape = \",df_out.shape)\n        return df_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d6a0e927fd517eb854554becff77a6ad8710fa0"},"cell_type":"code","source":"# Remove Outliers from each columns\n\ndf_clean = TurkyOutliers(train,'X1',True)\ndf_clean = TurkyOutliers(train,'X2',True)\ndf_clean = TurkyOutliers(train,'X3',True)\ndf_clean = TurkyOutliers(train,'X4',True)\ndf_clean = TurkyOutliers(train,'X5',True)\ndf_clean = TurkyOutliers(train,'X6',True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36572ffee8e39ffa851833331f21801ba6d9e0d3"},"cell_type":"code","source":"# Remove Outliers from each columns\n\ndf_clean_test = TurkyOutliers(test,'X1',True)\ndf_clean_test = TurkyOutliers(test,'X2',True)\ndf_clean_test = TurkyOutliers(test,'X3',True)\ndf_clean_test = TurkyOutliers(test,'X4',True)\ndf_clean_test = TurkyOutliers(test,'X5',True)\ndf_clean_test = TurkyOutliers(test,'X6',True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d26881bd43919b13ee86e8b34f507b3a73472f9"},"cell_type":"markdown","source":"Note : Removing outliers is not always a good idea. Depending on the problem - you treat outliers. We can leave the outliers but we would be limited to algorithms that are robust to outliers."},{"metadata":{"_uuid":"e22ce495426eafff33fcecac8ebd4dfa3282f8da"},"cell_type":"markdown","source":"## 2. Remove Correlated Features"},{"metadata":{"trusted":true,"_uuid":"2b93b5a78de280087f73a3c9bd0e5234b0c04e06"},"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = df_clean.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\nto_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"606105fc16cd4c4e92b1a96005c26b25b1a273b4"},"cell_type":"code","source":"# saving the labels\ny = train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eccd4ef8786d4517be9e26ca36797f8151b793d3"},"cell_type":"code","source":"# dropping the correlated features\n\ntest = test.drop(columns = to_drop)\nto_drop.append('target')\ntrain = train.drop(columns = to_drop)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6af3835fa37580ed4c0215d9985b5441aa917b12"},"cell_type":"markdown","source":"## 3. Scaling Features"},{"metadata":{"trusted":true,"_uuid":"0295655d736bad8ca3d0596bdbd853f087484288"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n# Create a minimum and maximum processor object\nscaler = StandardScaler()\n\n# Create an object to transform the data to fit minmax processor\ntrain_df = scaler.fit_transform(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"214cdb5602e8ebfb2cff89a17f94dcb52b510681"},"cell_type":"code","source":"test_df = scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"005aa6808fff02dcc6314daabbef51fbe88afaf1"},"cell_type":"code","source":"# Run the normalizer on the dataframe\ntrain_df = pd.DataFrame(train_df,columns=['X1','X4','X5','X6','I1','I2','I5','I6'])\n# Run the normalizer on the dataframe\ntest_df = pd.DataFrame(test_df,columns=['X1','X4','X5','X6','I1','I2','I5','I6'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75245a8430656d5fef3e5dcf7e48d19e423eed4b"},"cell_type":"markdown","source":"## 4. Model Building"},{"metadata":{"trusted":true,"_uuid":"b53824a5d02c72fde07768d5c23520252427fec8"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import cross_val_score\n\n# Model imports\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Custom scorer for cross validation\nscorer = make_scorer(f1_score, greater_is_better=True, average = 'micro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e02728a555dcc2917016b8a05597bd4df50cb6e"},"cell_type":"code","source":"features = list(train_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"828d0ca61d0cc05f3dd8e86610ea8eedbeaa63d9"},"cell_type":"markdown","source":"### 4.1 Random Forest Model"},{"metadata":{"trusted":true,"_uuid":"a5649662c439fe874fa1c9b3b099e14c89f39f73"},"cell_type":"code","source":"%%time\nmodel = RandomForestClassifier(n_estimators=100, random_state=10, \n                               n_jobs = -1)\n# 10 fold cross validation\ncv_score = cross_val_score(model, train_df, y, cv = 5, scoring = scorer)\n\nprint(f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3169b5e25897b16a9a9c2c906f1af9d8f31d281c"},"cell_type":"code","source":"model.fit(train_df, y)\n\n# Feature importances into a dataframe\nfeature_importances = pd.DataFrame({'feature': features, 'importance': model.feature_importances_})\nfeature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99277d68d69575d1b7b92e9af569bb1e6620bf11"},"cell_type":"code","source":"# Plot feature importance\nfeature_importance = model.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, train_df.columns[sorted_idx])#boston.feature_names[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"209e4c8e0e4d97d6f6688a66aa186bb27f876cab"},"cell_type":"code","source":"import warnings \nfrom sklearn.exceptions import ConvergenceWarning\n\n# Filter out warnings from models\nwarnings.filterwarnings('ignore', category = ConvergenceWarning)\nwarnings.filterwarnings('ignore', category = DeprecationWarning)\nwarnings.filterwarnings('ignore', category = UserWarning)\n\n# Dataframe to hold results\nmodel_results = pd.DataFrame(columns = ['model', 'cv_mean', 'cv_std'])\n\ndef cv_model(train, train_labels, model, name, model_results=None):\n    \"\"\"Perform 10 fold cross validation of a model\"\"\"\n    cv_scores = cross_val_score(model, train, train_labels, cv = 10, scoring=scorer, n_jobs = -1)\n    print(f'10 Fold CV Score: {round(cv_scores.mean(), 5)} with std: {round(cv_scores.std(), 5)}')\n    \n    if model_results is not None:\n        model_results = model_results.append(pd.DataFrame({'model': name, \n                                                           'cv_mean': cv_scores.mean(), \n                                                            'cv_std': cv_scores.std()},\n                                                           index = [0]),\n                                             ignore_index = True)\n\n        return model_results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66ac0db0323b4d85ea7200a75fc756644c9e1ffc"},"cell_type":"markdown","source":"### 4.2 Linear Support Vector Classifier"},{"metadata":{"trusted":true,"_uuid":"c7240f86bf0973eb5892227ca7c399f81df23717"},"cell_type":"code","source":"model_results = cv_model(train_df, y, LinearSVC(), \n                         'LSVC', model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09cf97dd4166a84ceadf06ae92a1fe36142a896b"},"cell_type":"markdown","source":"### 4.3 Gaussian Naive Bayes"},{"metadata":{"trusted":true,"_uuid":"262bfe9bb5aee39184e6521c5335381c0374a3d2"},"cell_type":"code","source":"model_results = cv_model(train_df, y, \n                         GaussianNB(), 'GNB', model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a0c84845da53cc7a66dfa4e4918b3c874bad8cd"},"cell_type":"markdown","source":"### 4.4 Linear Discriminant Analysis"},{"metadata":{"trusted":true,"_uuid":"57b1ed859b353427bbbeee537a4326220fc54eb8"},"cell_type":"code","source":"model_results = cv_model(train_df, y, \n                          LinearDiscriminantAnalysis(), \n                          'LDA', model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a203c056990f2f960f22b742d5b1460377b14e65"},"cell_type":"markdown","source":"### 4.5 K Nearest Neighbour"},{"metadata":{"trusted":true,"_uuid":"a3c91db1d072bb6ac90b1cbd827998d2abf34409"},"cell_type":"code","source":"for n in [5, 10, 20]:\n    print(f'\\nKNN with {n} neighbors\\n')\n    model_results = cv_model(train_df, y, \n                             KNeighborsClassifier(n_neighbors = n),\n                             f'knn-{n}', model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dc85933b018fa8b5d1296a8e3f5dfd4c0a311a3"},"cell_type":"markdown","source":"### 4.6 Extra Trees Classifier"},{"metadata":{"trusted":true,"_uuid":"dc3472e0e71b627ffdccefc21021b0e39fa11298"},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel_results = cv_model(train_df, y, \n                         ExtraTreesClassifier(n_estimators = 100, random_state = 10),\n                         'EXT', model_results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76d6979fb43906c1aedcaa32559cc52f8f404f34"},"cell_type":"markdown","source":"### 4.7 Random Forest Classifier"},{"metadata":{"trusted":true,"_uuid":"751d6bfa5698d989d3716c03e5416d33125c352f"},"cell_type":"code","source":"model_results = cv_model(train_df, y,\n                          RandomForestClassifier(150, random_state=10),\n                              'RF', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75c1d9f573a10e23f5009a987939a6126168c0eb"},"cell_type":"code","source":"model_results.set_index('model', inplace = True)\nmodel_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n                                  yerr = list(model_results['cv_std']),\n                                 edgecolor = 'k', linewidth = 2)\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');\nmodel_results.reset_index(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d4d1fc3a12031afff4c9ac8c927e57a869909c4"},"cell_type":"code","source":"model_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b353b34e6c35c8ab6fff0beea5017588e7915d1a"},"cell_type":"code","source":"clf = RandomForestClassifier(100, random_state = 10)\nclf.fit(train_df,y) # Its always a good idea to use the whole training set.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67765656f89354677d29dfccfc6f555d3d53b5ad"},"cell_type":"code","source":"# Predict of test data\npredict = clf.predict(test_df) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7005d92ad5c64f0bcfe6071c5a0b2df34d74c390"},"cell_type":"code","source":"test_df['target'] = predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ba29745652d6f85bad7b78c1257bed6b346ceff"},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ea920510c952c25b1a5cd90eeeda82d7cf2a180"},"cell_type":"code","source":"# Create the output file\n# Naming the output file as model number - model used - estimators\n\ntest_df.to_csv(\"01-rf_100.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c10fdb4299643cb767eff97ba74ccbee6cb8e26d"},"cell_type":"code","source":"# ! pip freeze > requirements.txt # To generate requirements file for reproducibility.\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6471902e52143376fcf53bd9b604fa53512f8877"},"cell_type":"markdown","source":"**Conclusion:**\n\n* We achieved micro F1-score of 0.956 using random forest model. For more details check the brief documentation in socialcops/docs folder.\n* Tree based models are more suitable for this problem."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}